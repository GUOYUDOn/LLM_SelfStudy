### 1. LLaMA系列

#### LLaMA1

- **模型规模**：7B，13B，33B，65B    [[2302.13971\] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

- **Normalization**：使用 **RMSNorm**（Root Mean Square LayerNorm）替代 LayerNorm。RMSNorm 并不减去均值，而只使用输入的 RMS（均方根）值来归一化。对于给定输入向量 $x \in \mathbb{R}^d$，其 RMSNorm 表达式为：
  $$
  \text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot \gamma
  $$
  其中：
  $$
  \text{RMS}(x) = \sqrt{ \frac{1}{d} \sum_{i=1}^d x_i^2 + \epsilon }
  $$
  $\gamma \in \mathbb{R}^d$：可学习的缩放参数，$\epsilon$：防止除以 0 的小常数（如 $1e{-8}$）。没有计算均值使得计算成本更低，在训练大模型时更稳定。

- **激活函数**：使用 **SwiGLU** 激活函数（替代 ReLU/GELU）。$x_1, x_2$ 是通过前置线性层得到的两个分支，
  $$
  \text{SwiGLU}(x_1, x_2) = x_1 \cdot \text{Swish}(x_2)
  $$

​	Swish 是：
$$
\text{Swish}(x) = x \cdot \sigma(x)
$$
​	$\sigma(x)$​ 是 Sigmoid 函数。完整的公式是：
$$
\text{SwiGLU}(x) = \text{Linear}(x) \cdot \text{Swish}(\text{Linear}(x))
$$
​	它具有更强的非线性表达能力，能够更稳定的传递梯度，结合了 Swish 的非线性与门控机制（GLU），是现	代大模型 FFN 层的首选激活方式。

- **位置编码**：使用 RoPE（Rotary Positional Embedding），支持任意上下文长度扩展（训练 2k，推理可以 4k 或更高）。
- **注意力机制**：标准多头自注意力，不使用 FlashAttention。Masked Multi-Head Self-Attention + 前馈神经网络（FFN）。
- **没有使用 Dropout**：在大数据量训练下 Dropout 效果边际降低，移除 Dropout 可提升训练速度和稳定性。
- **部分参数共享**：Embedding 和输出层权重共享，$W \in \mathbb{R}^{|V| \times d_{model}}$，能够减少参数量（尤其词表很大时节省显著），训练更稳定。共享权重 = 强约束，有助于模型学到一致的输入——输出映射。
- **学习率策略**：使用 bf16；使用层级学习率衰减（Layer-wise LR decay），靠近输入的层使用较小的学习率，靠近输出的层使用较大学习率，稳定深层训练。

#### LLaMA2

- **模型规模**：7B，13B，70B           https://arxiv.org/abs/2307.09288
- **模型架构**：依旧使用 decoder-only Transformer，沿用 **SwiGLU + RoPE + RMSNorm**。高参数模型开始使用 GQA。
- **Tokenization**：32K vocab，与 LLaMA1 一致，依旧使用 **SentencePiece**。
- **上下文窗口**：LLaMA 1 上下文窗口为 2048，LLaMA 2 统一扩展为 4096 tokens。
- **LLaMA 2-Chat**：有三阶段流程：1）SFT；2）Reward Modeling（RM）；3）Proximal Policy Optimization（PPO）。

#### LLaMA3

- **模型规模**：8B，70B                     https://arXiv:2407.21783v3
- **模型架构**：使用 **GQA** ，将 query heads（32个） 分成若干组，每组共享一组 key/value（8个），节省了参数量，并显著减少推理时 KV 缓存的显存占用，提高推理速度。
- **上下文窗口**：初始预训练在 8K tokens，后续使用 RoPE（θ=500,000） 进行持续训练以支持 128K tokens，使用文档级注意力掩码（document-level attention mask），避免序列中不同文档之间相互注意，增强训练稳定性。
- **训练策略**：预训练先使用8K tokens初始预训练，继续使用128K tokens做长上下文持续训练，最后退火训练（annealing）：只用高质量数据 & 学习率逐步减小。Post-Training时，采用 SFT + DPO 流程，不使用 RLHF。
- **Tokenization**：128K vocab，使用 **Byte-Pair Encoding (BPE)** 替代 SentencePiece， 基于OpenAI 的 `tiktoken` 分词器，扩充词汇表实现的。



### 2. Qwen系列

#### Qwen

- **模型版本**：base，chat，code，VL
- **Tokenization**：字节对编码（ byte pair encoding，BPE），扩展 `tiktoken` 分词器。
- **模型架构**：输入层与输出层不使用权重共享，采用非绑定的嵌入方法，使用两份独立的权重矩阵。位置编码采用 RoPE，使用FP32精度来表示逆频率矩阵。对于大多数层不使用偏置项，但在QKV层中添加了偏置，以增强模型的外推能力。使用 Pre-Normalization（前归一化），先进行层归一化再残差连接。使用 RMSNorm 作为 层归一化技术，并使用 SwiGLU 作为激活函数。使用 **MQA**，query分多头，但是共享一组KV的权重矩阵。FFN 的维度不是4倍，而是三分之八。
- **训练策略**：在注意力模块使用 Flash Attention，优化器选择 AdamW。
- **上下文全新注意力机制**：LogN-Scaling：将注意力中的点积部分）按一个缩放因子调整，让 softmax 的输出（注意力分布）的熵保持稳定。 Windowed Attention（窗口注意力）：将注意力限制在有限的上下文窗口内，低层（接近输入）仅能看到更小范围的 token，高层可以看到更大范围的 token。
- **对齐策略**：首先 SFT，之后 RLHF。先训练一个奖励模型（预训练+微调），之后进行 PPO， 

#### Qwen2.5

- **模型版本**：base，turbo， plus

- **Tokenization**：Byte-Level Byte Pair Encoding (BBPE)，使用自研的 Qwen Tokenizer。

- **模型架构**：使用 **GQA**，28 Q 头 / 4 KV 头（7B 模型）。使用 SwiGLU 作为激活函数。使用 RoPE 作为旋转位置编码，使用 YARN 技术将实际的 position 映射到较小的范围，使高维位置旋转更加平稳。对 QKV 增加偏置项。使用 RMSNorm + Pre-Norm 架构。

- **超长上下文**：使用 Dual Chunk Attention（DCA）技术，将长文本切分成多个 Chunk，并对不同 Chunk 采用“全局概览 + 局部精读”两种注意力策略。局部注意力（Local Chunk Attention）：对每个 Chunk 内部进行 全连接 attention；全局注意力（Global Chunk Attention）：每个 Chunk 抽取一个代表，捕捉全局语义结构。
  $$
  \text{Output} = \text{LocalAttention}(x) + \text{GlobalAttention}(x)
  $$

- **后训练**：1）SFT；2）离线强化学习（Offline RL）：DPO；3）在线强化学习（Online RL）：GRPO；4）长上下文微调：2阶段混合 SFT。

- **MoE**：在Qwen2.5-Turbo中使用moe模型，将部分 FFN 层替代为 MoE 层。1）Fine-grained Expert Segmentation（细粒度专家切分）：将专家划分为多个子组；2）Shared Experts Routing（共享专家路由机制）：允许在多个位置复用专家；3）Sparse Activation（Top-k Routing）：每个 token 只路由到 Top-2 专家；4）Load Balancing Loss（负载均衡损失）：
  $$
  L_{\text{balance}} = \text{KL}(\text{actual usage} \parallel \text{uniform distribution})
  $$



### 3. GPT系列

#### GPT1与GPT2

- **模型架构**：基于 Transformer 的 decoder-only 架构，GPT1使用 post-norm，先残差连接再归一化，GPT2 使用 pre-norm；采用 GELU 作为激活函数；使用Layer Normalization 进行归一化操作；优化器使用 Adam。
- **Tokenization**：Byte-Level BPE，`tiktoken`
- **训练方式**：GPT1：先无监督预训练，后在指定任务上微调；GPT2：预训练，zero-shot，进行多任务学习，通过一种通用的概率形式来刻画不同任务的输出，将输入、输出和任务都以自然语言的形式进行描述。

#### GPT3

- **模型架构**：与GPT2一致
- **Attention**：采用稀疏注意力机制，交替使用局部带状稀疏（locally banded sparse）注意力，仅考虑相对距离 k 之内的 token 和部分其他 token。
- **训练方式**：预训练 + few-shot

#### GPT4

- **MoE**：模型包含多个专家子网络（如数千个），每个输入token根据其内容由门控网络（Gating Network）选择激活少量专家。
- **多模态**
- **模型架构**：使用 GQA。
- **训练方式**：SFT + RLHF



### 4. BERT

- **模型架构**：主要使用 Transformer 的 Encoder 标准架构，Post-Norm 。输入嵌入分为三个部分：Token Embedding、Segment Embedding（区分句子）和 Position Embedding。激活函数使用 GELU，归一化函数使用 LayerNorm；可学习的绝对位置编码。
- **预训练**：1）Masked Language Model (MLM)：从输入中随机 mask 15% 的 token，80% 替换为 `[MASK]`，10% 替换为随机词，10% 保留原词，要预测这些 masked token 的原始词，学习双向上下文表征。2）Next Sentence Prediction ：判断 B 是否为 A 的下一句。
- **微调**：1）文本分类：用 `[CLS]` token 的输出接一个全连接层；2）命名实体识别（NER）：对每个 token 做分类；3）文本蕴含（NLI）：句子对输入，`[CLS]` 做判断；4）问答（SQuAD）：预测答案的起始和结束位置。
- **Tokenization**：WordPiece Tokenizer



### 5. T5（Text-To-Text Transfer Transformer）

- **模型架构**：标准的 Transformer Encoder-Decoder 架构，使用 LayerNorm 作为归一化函数，采用 pre-norm 架构；使用 ReLU 作为 FFN 的激活函数；使用可学习的相对位置编码，Learned Relative Position Bias，在 Attention 中添加相对位置信息。
- **Tokenization**：SentencePiece + Unigram Model
- **预训练**：Span Corruption（span-mask denoising）：随机遮盖句子的若干连续片段（span），让模型学会“填空”。
- **微调**：SFT，每个任务都定义成一个文本生成任务，输入中包含任务提示词（task prefix）。













