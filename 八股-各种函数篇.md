### 1. 讲一下 BatchNorm 和 LayerNorm

- **BatchNorm**

Batch Normalization（批归一化），是对**同一批次（batch）内不同样本的同一特征维度**进行归一化。假设输入数据为：$X \in \mathbb{R}^{N\times D}$，N为批大小，D为特征维度。对每个特征维度，在batch内归一化：
$$
\mu_{BN} = \frac{1}{N}\sum_{i=1}^{N} x_i,\quad
\sigma_{BN}^2 = \frac{1}{N}\sum_{i=1}^{N}(x_i-\mu_{BN})^2
$$

$$
\hat{x}_i = \frac{x_i-\mu_{BN}}{\sqrt{\sigma_{BN}^2+\epsilon}}
$$

其中，$\epsilon$ 是一个极小常数（如 $10^{-5}$），防止除以零。在归一化之后，为了防止特征分布过于僵硬，引入两个可学习参数 $\gamma$（尺度因子）和 $\beta$（偏移量）：
$$
y^{(i)} = \gamma\hat{x}^{(i)}+\beta
$$
其中 $\gamma$、$\beta$ 随网络一起训练，并通过反向传播自动更新。

- **LayerNorm**

Layer Normalization（层归一化），是对**单个样本的所有特征维度**进行归一化。假设输入数据为：$X \in \mathbb{R}^{N\times D}$​，N为批大小，D为特征维度。LN 对单个样本的全部特征维度归一化，对于每个样本：
$$
\mu_{LN} = \frac{1}{D}\sum_{j=1}^{D} x_j,\quad
\sigma_{LN}^2 = \frac{1}{D}\sum_{j=1}^{D}(x_j-\mu_{LN})^2
$$

$$
\hat{x}_j = \frac{x_j-\mu_{LN}}{\sqrt{\sigma_{LN}^2+\epsilon}}
$$

在归一化之后，为了防止特征分布过于僵硬，引入两个可学习参数 $\gamma$（尺度因子）和 $\beta$（偏移量）：
$$
y_i = \gamma \hat{x}_i + \beta
$$
其中 $\gamma$、$\beta$ 随网络一起训练，并通过反向传播自动更新。

- **BN 与 LN 比较**

BN 比较适合用于 CNN，LN 主要用于 Transformer 和 RNN 等；BN 赖于批内多个样本的统计特性，对批量大小要求较高，小批量通常不行，而 LN 归一化单个样本本身，无论 batch size 多小（甚至为1），效果都很稳定。此外，BN 在序列维度上归一化不合理，因为不同位置可能表示截然不同的语义信息，LN 逐样本逐层地对特征进行归一化，更适合 Transformer 的序列数据。





### 2. 讲一下 RMSNorm

RMSNorm (Root Mean Square Normalization，均方根归一化) 是一种比 LayerNorm (LN) 更简单的归一化技术。RMSNorm 只使用数据的均方根 (RMS) 进行归一化，而不使用均值（mean）。

对于输入的向量：
$$
\mathbf{x} = (x_1, x_2, \dots, x_D)
$$
我们首先需要计算它的均方根：
$$
\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{D}\sum_{i=1}^{D}x_i^2 + \epsilon}
$$
之后不使用均值，直接进行归一化处理：
$$
\hat{x}_i = \frac{x_i}{\text{RMS}(\mathbf{x})}
$$
和 LN 一样，引入一个可学习的缩放参数 $\gamma$，但一般省略了位移参数 $\beta$：
$$
y_i = \gamma_i \hat{x}_i
$$
相比 LN，RMSNorm 去掉了 LayerNorm 中冗余的均值操作，以更低的计算成本实现了相当甚至更好的归一化效果，非常适合大模型中的高频归一化任务。此外，LN 的去均值操作可能引起训练时特征分布的“均值漂移”（mean-shift），RMSNorm 仅基于特征的幅度（模长）而非均值，因此避免了这种漂移问题，更稳定。





### 3. 讲一下 Sigmoid 函数和 Softmax 函数

- #### Sigmoid

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**优点**：输出值可解释为概率；光滑、连续、可导。

**缺点**：容易梯度消失，尤其是在两端。

**特点**：它的输入和输出都是标量，通常用于二分类问题。

**导数**：
$$
\frac{d}{dx} \sigma(x) = \sigma(x)(1 - \sigma(x))
$$

- #### Softmax

Softmax 是作用于一个向量 $\mathbf{z} = [z_1, z_2, ..., z_K]$ 上的函数，将一组实数转化为一组和为 1 的概率分布：
$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}=y_i, \quad i = 1, ..., K
$$
**优点**：输出可解释为类别概率分布；能凸显最大值，提升分类信号。

**缺点**：当输入非常大时，数值可能不稳定（需做 log-sum-exp 稳定化）

**特点**：输入与输出都是向量，常用于多分类问题。

**梯度**：：
$$
\frac{\partial y_i}{\partial z_j} =
\begin{cases}
y_i(1 - y_i), & \text{if } i = j \\
- y_i y_j, & \text{if } i \ne j
\end{cases}
$$
令 $\mathbf{y} = \text{softmax}(\mathbf{z})$，则：
$$
\frac{\partial \mathbf{y}}{\partial \mathbf{z}} = \text{diag}(\mathbf{y}) - \mathbf{y} \mathbf{y}^T
$$




### 4. 讲一下交叉熵损失

- #### 二元交叉熵 Binary Cross Entropy（BCE）

二元交叉熵通常来说是交叉熵在二分类下的特例。对于单个样本：
$$

\text{BCE}(y, \hat{y}) = - \left[ y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \right]
$$
其中：$y \in \{0, 1\}$：真实标签（0 或 1）；$\hat{y} \in (0, 1)$：模型的预测概率（通常来自 sigmoid）。

**数学推导**：从最大似然估计的角度出发，对于 $y \sim \text{Bernoulli}(\hat{y})$，其对数似然为：
$$
\log P(y|\hat{y}) = y \log \hat{y} + (1 - y) \log (1 - \hat{y})
$$
因此最小化负对数似然的结果是：
$$
\mathcal{L}_{\text{BCE}}(y, \hat{y}) = - [ y \log \hat{y} + (1 - y) \log(1 - \hat{y}) ]
$$
**为什么二元交叉熵和 Sigmoid 很配？**
$$
\mathcal{L}(z, y) = - \left[ y \log \hat{y} + (1 - y) \log(1 - \hat{y}) \right]
$$
是我们定义的二元交叉熵，使用链式法则进行求导：
$$
\frac{d\mathcal{L}}{dz} = \frac{d\mathcal{L}}{d\hat{y}} \cdot \frac{d\hat{y}}{dz}
$$
而 BCE 的导数是：
$$
\frac{d\mathcal{L}}{d\hat{y}} = -\left( \frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}} \right)
$$
将 Sigmoid 带入：
$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}} \Rightarrow \frac{d\hat{y}}{dz} = \hat{y}(1 - \hat{y})
$$
应用到链式求导中：
$$
\frac{d\mathcal{L}}{dz} = \frac{d\mathcal{L}}{d\hat{y}} \cdot \frac{d\hat{y}}{dz}
= -\left( \frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}} \right) \cdot \hat{y}(1 - \hat{y})=\hat{y}-y
$$
换句话说，：
$$
\frac{d\mathcal{L}}{dz} = \hat{y} - y = \sigma(z) - y
$$
这个简洁的梯度表达式在反向传播中极其高效，同时也能直接反应真实标签和估计标签的距离。

- #### 交叉熵 Cross Entropy

交叉熵用于衡量两个概率分布之间的差异，定义为：
$$
H(y, \hat{y}) = - \sum_{i=1}^{K} y_i \log \hat{y}_i
$$
其中，$y = [y_1, y_2, ..., y_K]$ 表示真实分布（通常是 one-hot 向量），$\hat{y} = [\hat{y}_1, \hat{y}_2, ..., \hat{y}_K]$ 则代表预测分布（如 softmax 输出）。

**数学推导**：对于多分类问题，与二分类类似地，我们可以建模为：
$$
P(y|\hat{y}) = \prod_{i=1}^{K} \hat{y}_i^{y_i}
$$
取对数，得到负对数似然（交叉熵），最大似然估计等价于最小化负对数似然：
$$
\mathcal{L}_{\text{CE}}(y, \hat{y}) = -\sum_{i=1}^{K} y_i \log \hat{y}_i
$$
**为什么交叉熵与 Softmax 很搭？**

我们直接将 Softmax 的函数带入交叉熵中，并化简：
$$
\begin{align}
\mathcal{L}(z, y) &= - \sum_{i=1}^{K} y_i \log(\hat{y}_i)
= - \sum_{i=1}^{K} y_i \log \left( \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \right) \\
&= - \sum_{i=1}^{K} y_i \left( z_i - \log \sum_{j=1}^{K} e^{z_j} \right)
= - \sum_{i=1}^{K} y_i z_i + \log \sum_{j=1}^{K} e^{z_j}
\end{align}
$$
接下来对 $z_i$ 求导：
$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial z_i}
 & = \frac{\partial}{\partial z_i} \left( - \sum_{k=1}^{K} y_k z_k + \log \sum_{j=1}^{K} e^{z_j} \right) \\
 &= - y_i + \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} = \hat{y}_i - y_i
\end{align}
$$
这个结果意味着**softmax + cross entropy** 的组合，在反向传播中不需要显式计算 softmax 的 Jacobian 矩阵，同时结果也与预期相同，非常高效、简洁。





### 5. 讲一下其他常见的激活函数

1. **ReLU**（Rectified Linear Unit）
   $$
   \text{ReLU}(x) = \max(0, x)
   $$
   训练速度快，已成为默认激活函数；不过存在神经元死亡问题：如果输入为负，梯度为0，可能永远不会更新（dead neuron）。

2. **Leaky ReLU**
   $$
   \text{LeakyReLU}(x) =
   \begin{cases}
   x, & x > 0 \\
   \alpha x, & x \leq 0
   \end{cases}
   $$

​	通常 $\alpha = 0.01$，避免神经元死亡。

3. **GELU**（Gaussian Error Linear Unit）
   $$
   \text{GELU}(x) = x \cdot \Phi(x)
   $$

​	其中 $\Phi(x)$ 是标准正态分布的累积分布函数。但通常写作它的近似版本：
$$
\text{GELU}(x) \approx 0.5x \left[1 + \tanh\left(\sqrt{\frac{2}{\pi}} \left(x + 0.044715 x^3\right)\right)\right]
$$

4. **Tanh**（双曲正切函数）
   $$
   \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
   $$

​	输出范围：$(-1, 1)$；零中心，较 sigmoid 更稳定，但仍然存在梯度消失问题（两端饱和）。

5. **Swish**
   $$
   \text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
   $$

​	$\sigma(x)$ 是 Sigmoid 函数。Swish 是平滑的、非单调的激活函数，实测优于 ReLU。

6. **SwiGLU**（Swish-Gated Linear Unit）

​	给定输入向量 $x \in \mathbb{R}^{d}$，首先经过一个全连接层（线性层），映射为维度为 $2d$ 的向量：
$$
h = W x + b \in \mathbb{R}^{2d}
$$
​	然后我们将这个 $2d$-维向量按通道切成两半：
$$
[x_1, x_2] = \text{split}(h), \quad x_1, x_2 \in \mathbb{R}^{d}
$$
​	之后计算：
$$
\text{SwiGLU}(x) = x_1 \cdot \text{Swish}(x_2) = x_1 \cdot \left( x_2 \cdot \sigma(x_2) \right)
$$
​	SwiGLU 结合 Swish 的平滑性和门控机制的表达力，可训练性好，梯度更稳定，实际效果更好。