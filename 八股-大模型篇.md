### 1. 大模型的架构有哪几种？有什么代表？有什么优缺点？

#### --- 自回归（Auto-Regressive）架构

**Decoder-Only**：自回归（AR）模型按时间顺序逐步生成序列的每个元素。它基于因果注意力（Causal Attention），只能使用已生成的内容来预测下一个 token，因此特别适合文本生成任务。

- **工作机制**

1）预训练时，采用自回归语言建模（Auto-Regressive LM），仅预测下一个token：
$$
P(w_t | w_1,w_2,...,w_{t-1})
$$
其中，$w_t$ 表示当前 token，前面的 tokens 作为上下文。

2）训练使用Transformer Decoder，采用 Masked Self-Attention，防止模型看到未来的信息。计算损失时，只考虑预测的 token，不考虑输入部分的 token。

3）生成时，通常使用采样（sampling）、Top-k 采样或温度调节方式逐步生成文本。

- **代表模型**

GPT 系列（GPT-2, GPT-3, GPT-4, GPT-4 Turbo）

LLaMA 系列（LLaMA, LLaMA 2, LLaMA 3）

Qwen 系列（Qwen-7B, Qwen-14B）

- **优点**

1）生成能力强：擅长自由文本生成，如对话、写作、代码生成等；

2）能处理长距离依赖：相比传统 RNN，Transformer 结构的全局注意力能捕捉更长的上下文。

3）预训练迁移性强：可以通过微调（fine-tuning）适应各种任务，如代码生成、对话 AI、文学创作。

- **缺点**

1）推理速度慢：由于是逐步生成（one-by-one decoding），并行化能力有限。

2）计算开销大：大规模模型推理时，每生成一个新 token 都需要重新计算整个上下文，成本较高。

3）无法双向建模：仅能看到过去的 token，无法看到后续信息（适用于生成但不适用于理解）。



#### --- 自编码（Auto-Encoding）架构

**Encoder-Only**：自编码（AE）模型采用双向注意力（Bidirectional Attention），可以同时利用文本的前后文信息进行建模，因此特别适用于文本理解任务（如分类、情感分析、问答等）。

- **工作机制**

1）预训练时，使用 Masked Language Model（MLM），在输入文本中 **随机遮蔽（mask）部分 token**，让模型预测这些被隐藏的 token：
$$
P(w_i|w_1,...,w_{i-1},w_{i+1},...,w_n)
$$
2）训练后可微调（fine-tune）到不同的任务，如分类、命名实体识别（NER）、问答（QA）等。

- **代表模型**

BERT（Bidirectional Encoder Representations from Transformers）

RoBERTa（Robustly Optimized BERT Approach）

- **优点**

1）文本理解能力强：适用于分类、情感分析、问答、信息提取等任务。

2）双向注意力：能同时利用上下文，信息利用率比自回归模型更高。

3）训练效率高：相比自回归模型，MLM 训练方式能更快地学习文本信息。

- **缺点**

1）不能用于文本生成，因为它的训练目标是填充缺失的 token，而不是自回归生成。

2）受限于最大输入长度（一般 BERT 只能处理 512 个 token，Longformer 等模型可以扩展到更长文本）。



#### ---Encoder-Decoder（Seq2Seq）架构

**Encoder-Decoder** 结构结合了自编码（理解）+ 自回归（生成），适用于文本转换任务（如机器翻译、文本摘要、问答等）。

- **工作机制**

1）Encoder（编码器）：采用 BERT-style 双向注意力，用于理解输入文本的含义。提取输入序列的全局表示，作为 Decoder 的初始状态。

2）Decoder（解码器）：采用 GPT-style 自回归结构，通过 因果注意力（Causal Attention） 逐步生成输出序列。生成过程中，每次仅使用已生成的 token 预测下一个 token。

3）训练方式：BART 采用 去噪自编码（Denoising Autoencoder），对输入句子进行扰动（如删除、遮蔽部分词），然后让模型恢复原始文本。T5 采用 文本到文本转换（Text-to-Text） 方式，即所有 NLP 任务都转化为 "输入 → 输出" 任务。

- **代表模型**

T5（Text-to-Text Transfer Transformer）

BART（Bidirectional and Auto-Regressive Transformers）

- **优点**

1）适用于序列转换任务：如机器翻译、摘要生成、问答等。

2）可同时用于理解和生成：Encoder 负责理解，Decoder 负责生成。

3）统一框架：T5 统一了 NLP 任务，所有任务都用 "文本到文本" 方式处理，易于迁移到不同任务。

- **缺点**

1）计算成本更高：同时使用 Encoder 和 Decoder，计算量比纯 BERT 或 GPT 大。

2）训练和微调较复杂：需要设计输入格式，不同任务可能需要不同的损失函数。

3）推理速度慢于 GPT：仍然需要逐步解码输出，难以并行加速。



### 2.  什么是因果注意力（Causal Attention）？

因果注意力（Causal Attention）是一种特殊的自注意力机制，用于自回归（Auto-Regressive）模型（即 Decoder-Only 结构），其核心目的是保证当前 token 只能看到过去的 token，而不能看到未来的 token，从而保持因果关系，使得生成时每个 token 只能依赖之前的 token 进行预测。

使用下三角 Mask 矩阵（Causal Masking），行代表当前处理的 token。注意力分数计算公式：
$$
A = \text{softmax}(\frac{QK^T}{\sqrt{d_k}} + M)
$$



### 3. 为什么大模型更喜欢decoder-only架构？

Decoder-Only 架构仅使用 Decoder 结构，结构更简单，只需计算一次自注意力（Self-Attention），不涉及交叉注意力（Cross-Attention），训练开销更低，尤其在大规模预训练阶段，资源利用率更高。而且目前大模型的主流任务为文本生成任务，非常适合 Decoder-Only 模型的自回归（Auto-Regressive）机制。有很强的通用性和零样本生成能力，迁移能力与泛化性很好，有统一的 Prompt 格式，适合指令微调。目前生态比较成熟，配合RAG更高效。

缺点是文本理解能力弱于 BERT，不能自然处理多模态输入。



### 4. 什么是大模型幻觉？解决方案是什么？

大模型幻觉（Hallucination） 指的是大语言模型（LLM）在生成文本时，生成了与事实不符、缺乏依据、甚至完全虚构的信息。

|                类型                 |                   描述                   |
| :---------------------------------: | :--------------------------------------: |
|  事实性幻觉（Fact Hallucination）   |          生成了不符合事实的内容          |
| 逻辑性幻觉（Logical Hallucination） |      逻辑推理错误，生成的结论不合理      |
| 引用幻觉（Reference Hallucination） |       生成不存在的文献、链接、法规       |
|   任务幻觉（Task Hallucination）    | 模型错误理解任务，生成了无关或错误的内容 |
| 数字幻觉（Numerical Hallucination） |        生成的数值、统计数据不准确        |

#### 幻觉的主要原因

- **训练数据的局限性**：训练数据可能包含错误信息、过时信息或不完整信息
- **模型的概率生成机制**：大模型的核心是基于概率的 token 预测，并不会主动检查其生成内容的真实性。
- **缺乏外部知识检索**：训练数据是静态的，无法访问最新信息。
- **长文本信息丢失**：长文本中，注意力机制会在上下文窗口外丢失信息，导致模型错误理解问题或自创答案。
- **对抗样本攻击**：模型对模棱两可或刻意引导性的问题（如反事实问题）会生成虚假的但看似合理的回答。

#### 解决大模型幻觉的方法

- **检索增强生成**：让 LLM 查询外部数据库，而不是仅依赖内部参数存储的知识。能够大幅减少幻觉，提升事实准确性。可动态更新知识，无需重新训练 LLM。但需要高质量的知识库，否则可能召回错误信息，而且检索和生成的效率受限于数据库规模和索引方式。
- **微调**：通过监督微调（SFT），让模型对历史幻觉数据进行修正。使用人类反馈强化学习（RLHF），让 LLM 更倾向于生成真实信息，而不是凭空编造。增量更新（如 LoRA 低秩适配），减少每次微调的计算开销。
- **基于事实的知识校验（Fact-Checking）**：使用 外部搜索引擎、数据库、API 自动检查 LLM 生成内容的真实性。
- **限制模型生成的温度参数**：降低温度，减少 LLM 随机性，让输出更稳定。
- **增强 LLM 对“不知道”的能力**：让模型在无知识时直接拒答。



### 5. 大模型的常见参数有哪些？有什么作用？

- **温度（Temperature）**

能够控制生成文本的随机性（高温度=更随机，低温度=更保守）。在 LLM 生成文本时，每个 token 的生成概率为
$$
P(w_i)=\frac{\text{exp}(s_i/T)}{\Sigma_j \text{exp}(s_j/T)}
$$
其中 $s_i$ 是 token $w_i$ 的 logits（未归一化分数）。

低温度（T → 0） → 选择概率最高的 token（趋于确定性）

高温度（T → 1） → 选择概率较低的 token 也有一定概率被选中（增加随机性）

-  **Top-k 采样（Top-k Sampling）**

仅从概率最高的前 k 个 token 中进行采样，减少低概率 token 的影响，提高生成质量。在每个生成步骤，排序所有可能的 token，根据概率最高的前 k 个 token 进行采样。低于 top-k 之外的 token 被直接过滤（概率设为 0）。

`top_k = 1, 5, 50`    完全确定性  → 生成更加自由

- **Top-p 采样（核采样，Nucleus Sampling）**

从累计概率超过 p% 的 token 中随机采样，保证既灵活又可控。计算所有 token 的 softmax 概率，并按照概率**从高到低**排序。选取累计概率达到 `p` 的前 N 个 token（N 是动态变化的）。

`top_p = 0.9` 表示：选择概率最高的 token，直到累积概率≥ 90%，然后从中采样。低概率 token 被忽略。

`top_p = 0.3 0.7 0.9`

- **Length Penalty（长度惩罚）**

控制生成文本的长度，防止生成过短或过长的文本。在文本生成任务（如翻译、摘要、法律文书生成）中，模型默认会倾向于生成短文本，因为短文本通常具有较高的归一化概率。为了避免过度偏向短文本，可以使用 Length Penalty 调整生成策略。

在 `Beam Search` 或 `Greedy Search` 生成方法中，模型计算序列得分：
$$
\frac{1}{T} \sum_{t=1}^T \log P(y_t|y_{1:t-1})
$$
其中 $T$ 是序列长度，$P(y_t)$ 是当前 token 的概率。当 `length_penalty` 被应用时，得分计算方式变为：
$$
\frac{1}{T^{\alpha}} \sum_{t=1}^{T} \log P(y_t | y_{1:t-1})
$$
其中：$\alpha$ 是 `length_penalty` 的值。当 $\alpha > 1$，长文本得分增加，鼓励生成更长文本。$\alpha<1$ 可用来生成摘要。



### 6. 大模型的参数是如何起作用的？

- 每一步生成时，模型预测所有 token 的 logits（向量$s$），它表示每个词的原始打分。

- 应用**温度**：
  $$
  s'_i = \frac{s_i}{\text{temperature}} 
  $$

- 筛选 **top_k** 个候选 token。

- 再应用 **top_p**，从累计概率达到 0.8 的子集中进一步采样。

- 从剩余的 token 中随机采样一个，作为下一个生成的 token。

-  length_penalty 影响整个生成句子的“得分”，而是影响是否提前停止生成或最终选哪条路径。



### 7. 常见的文本生成策略有哪些？

- **Sampling（采样式生成）** 最主流！

每一步不是取概率最大的 token，而是从概率分布中采样一个 token通过设置 `temperature`, `top_k`, `top_p` 来控制采样行为。生成内容多样、有创造力，适合写故事、对话、诗歌等开放式任务；速度快，结构简单。结果不稳定、质量波动大，容易出现语义重复、幻觉或跑题。

- **Beam Search（束搜索）**

在每一步不只考虑概率最大的 token，而是保留多个“候选路径”。每一步扩展所有候选路径，然后选择得分前 `beam_width` 条路径继续，最后选择得分最高的那一条作为输出。更稳定、更精确，适用于需要完整、严谨输出的任务（如翻译、法律、摘要），易与 `length_penalty` 搭配，避免过早输出短句子。

- **Contrastive Search（对比搜索）**

引入两阶段策略：第一步：从 top-k 候选中选概率最高的若干 token。

第二步：从中选出最少与上下文重复（最大“对比性”）的那个 token：
$$
\text{Score} = \lambda \cdot P(w) - (1 - \lambda) \cdot \text{Similarity}(w, \text{history})
$$
$\lambda$ 控制概率和重复度的平衡。兼顾准确性与多样性，有效避免重复、幻觉，实现复杂，速度略慢于 sampling，但比 beam 快。



### 8. 大模型的参数是怎么计算的？

参数是模型中的可训练权重数量，主要包括：

- Transformer 层中的 权重矩阵（线性变换、注意力权重等）
- Embedding 层的词向量表
- LayerNorm、MLP 等结构中的参数

1）Transformer层（考虑一层Attention）

**多头注意力**：包含三个投影：$W^Q, W^K, W^V$：每个维度 $d_{\text{model}} × d_{\text{model}}$；输出投影 $W^O$：$d_{\text{model}} × d_{\text{model}}$
$$
\text{Attention Params} = 4 × d_{\text{model}} × d_{\text{model}}
$$
**前馈网络**：包含两个线性层，每个线性层参数量为 $d_{model} \times d_{\text{ff}}$，$d_{\text{ff}}$ 一般是4倍的 $d_{model}$
$$
\text{FFN Params} = 2 × d_{\text{model}} × d_{\text{ff}} = 8 × d_{\text{model}}^2
$$
Transformer的总参数量约为：$12\times d_{\text{model}}^2$

2）LayerNorm等

每次层归一化有 $2\times d_{model}$ 的参数量，偏差项是 $常数 \times d_{model}$ 的参数量。（忽略不计）

3）Embedding 层

词向量矩阵：$V × d_{model}$，其中 $V $ 是词表大小，$d_{\text{model}}$ 是隐藏层维度。



### 9.  大模型的训练流程通常是怎样的？

- **预训练（Pretraining）**

这一阶段采用自监督学习（self-supervised learning），让大模型从原始数据中构造监督信号来进行训练。自回归语言建模（如 GPT）：预测下一个 token；MLM（如 BERT）：填空预测。本阶段目的是让模型具备语言建模能力（掌握基本语言规律、常识、语义结构等）。使用海量数据，训练时间极久。

- **对齐训练（Alignment）**

这一阶段让模型的输出更符合人类偏好，主要有三阶段。

1）**SFT**：监督微调（Supervised Fine-Tuning）

使用人工标注的指令-回答对，通过监督学习，Loss = CrossEntropy，引导模型学会遵循任务指令和回答风格。

2）**RLHF**：人类反馈强化学习（Reinforcement Learning with Human Feedback）

使用的数据为同一问题的多个回答 + 人类偏好排名，训练一个奖励模型（Reward Model），使用 PPO（Proximal Policy Optimization）优化生成策略，优化模型生成的答案。

3）**DPO**（Direct Preference Optimization）

直接优化模型使其偏向更受欢迎的回答

-  **微调（Fine-Tuning）**

让基础大模型在特定领域或任务上表现更好，使用领域数据和下游任务数据。常用方法有：全参数微调（Full Fine-Tune），LoRA / QLoRA（低秩适配），Prompt Tuning / Adapter 等轻量微调方法。

- **推理与部署（Inference & Serving）**

模型量化（8bit、4bit）降低显存占用；编译加速（TensorRT, vLLM, DeepSpeed-Inference）；分布式推理；多轮对话管理 / 检索增强（RAG）。



### 10. 大模型的微调方式有哪几种？

-  **全参数微调（Full Fine-tuning）**

更新模型中所有参数，能力最强，可实现最大性能提升。不过资源消耗大（显存、计算），易过拟合，尤其是在小数据集上。

-  **LoRA（Low-Rank Adaptation）**

冻结原始模型参数，仅在每个权重矩阵上添加一个“低秩可训练矩阵”。能够显著降低训练参数量，适配性强，易于部署。性能略低于全参数微调，但性价比极高。

- **提示微调（Prompt Tuning）**

固定预训练模型参数，仅学习一小组可训练的“软提示”（soft prompts），作为输入的一部分。参数量少、训练开销小。通用性较弱，效果依赖提示设计。

- **Prefix Tuning / P-Tuning**

类似 Prompt Tuning，但插入的是“可训练的向量前缀”，作为 Transformer 每层的输入。比 Prompt Tuning 表达能力更强。

- **Adapter 微调**

在 Transformer 的每层之间插入小型可训练网络（Adapter 模块），原始参数冻结。可扩展性强，多个 Adapter 可并存。模型体积略增，训练复杂性略高于 LoRA。

- **QLoRA（Quantized LoRA）**

在量化（通常为4-bit）模型上应用 LoRA 微调。能够极致节省内存，同时保留了良好的性能。效果一般，推理速度受限于量化精度。



### 11. Causal Attention 和 Prefix Attention 有什么区别？

Causal Attention 和 Prefix Attention 的主要区别在于注意力掩码（attention mask）不同，从而决定了模型在处理序列时，每个位置的 token 能“看到”哪些其他位置的 token。

- Causal Attention（因果注意力）：每个 token 只能看到自己及其左侧的 token，不能看到右边未来的信息。常用模型有：GPT、LLaMA
- Prefix Attention（前缀注意力）：前缀部分（如 prompt）对整个后续序列是完全可见的，但后续 token 之间仍然使用 causal attention（即不能看未来）。应用场景：P-tuning，LLaMA + Prefix-tuning



### 12. 大模型的嵌入层是怎么构建？

基础流程：输入句子 →  Tokenizer 分词，并将 token 映射到词汇表的唯一 id →  Embedding Layer 将 token id 转成向量。

嵌入层本质上是一个 **词表大小 × 嵌入维度的矩阵**，通常表示为：
$$
E \in \mathbb{R}^{V \times d}
$$

- $V$ 是词表大小（vocabulary size），比如 LLaMA 3 为 128,256
- $d$ 是嵌入维度，通常与隐藏状态维度相同（如 4096、8192）

输入的 token id 通过查表（查 $E$ 的第 i 行）得到嵌入向量。通常大模型的词嵌入和输出权重共享（Weight Tying），将嵌入层矩阵 $E$ 和输出层 softmax 权重 $W$ 设为相同矩阵（即 $W = E$​）。在输出时计算 token logits：
$$
\text{logits} = W h \in \mathbb{R}^{V}
$$
此时每个 logit 表示“当前输出是第 i 个 token 的非归一化分数”，通过不同的采样策略得到对应的 token。

如果有位置编码（传统 Transformer），输入的 token 经过 embedding 后加入位置编码后传入 Attention 层，注意 RoPE 没有加在 embedding层上，而是在每一层、每一个 attention head 中对 Q 和 K 做 RoPE 旋转。





### 13. 全参微调和PEFT有什么区别？

所以全参微调适用于追求极致性能的大规模训练，而 PEFT 方法如 LoRA、Adapter 则在资源受限、快速迁移、多任务部署场景下非常高效。



### 14. 详细介绍一下 LoRA（Low-Rank Adaptation）

- **原理**

![image-20250328143535410](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20250328143535410.png)

LoRA 主要应用于线性层，假设某个线性层为：
$$
y = W x
$$
LoRA 把这个 $W$ 分解成：
$$
W' = W + ΔW = W + BA
$$
其中，$W$：原始的权重矩阵（冻结，不训练）；$B \in \mathbb{R}^{d_{out} \times r}$：可训练的“上投影矩阵”；$A \in \mathbb{R}^{r \times d_{in}}$：可训练的“下投影矩阵”。重点要关注 $AB$​ 矩阵的**初始化**问题：

1. **A**：使用标准的权重初始化方法（如 Xavier 或 Kaiming）

2. **B**：初始化为全零矩阵

初始化的原因如下：LoRA 的设计初衷是对原始模型进行“扰动式适配”，不能一开始就干扰原始模型的行为。而在初始化时：
$$
\Delta W = B \cdot A = 0 \cdot A = 0
\Rightarrow W' = W
$$
因此刚开始模型行为**完全一致于原始模型**，不会破坏原有语义能力。随着训练进行，$B$ 逐步学出任务相关方向，开始提供增益。如果 $B$ 用随机值初始化，模型一开始就等于被“扰动”，可能导致性能震荡、训练不稳定。

- **哪些矩阵可以 LoRA 微调？**

理论上讲所有线性变化都可以使用 LoRA 微调，在 Transformer 架构下的矩阵中，$W^Q,W^K,W^V,W^O$ 以及 FFN 层的上下投影矩阵、门控矩阵理论都可以用 LoRA 微调。一般不考虑修改Embedding 层和 LayerNorm 层。在实际中，一般仅对  $Q$、$V$ 的投影矩阵微调，可以达到最好的效果。（我的微调默认使用了全部7个矩阵）

- **参数怎么设置？**

超参数一般有 r 和 lora_alpha。r 是低秩的大小，一般设置为 4，8，16，太大的话容易过拟合。
$$
W' = W + \underbrace{\frac{\alpha}{r} A B}_{\text{LoRA扰动项}}
$$
lora_alpha 控制扰动的强度，一般设置为 r 大小的两倍。最常用的组合是 r=8, lora_alpha=16。

对于 7B 模型的微调，LoRA 微调部分仅引入了 20M ⇒ 占比约 0.28%





### 15. 介绍一下 LoRA 的变体

- **AdaPtLoRA**

AdaPtLoRA = Adaptive Prompt-Tuning + LoRA，可以为不同任务自适应选择 LoRA 插入位置，同时引入任务 prompt 作为引导。它将 Prompt Tuning 和 LoRA 两种方法结合，尤其适合多任务、多领域、多语言的场景。

对于每个任务 $t$，定义一个任务特定的 Prompt 向量 $P_t \in \mathbb{R}^{l_p \times d}$。$l_p$：Prompt 长度；$d$：模型 hidden size；Prompt 被拼接到输入上作为前缀输入 token：
$$
X_{\text{input}}^{(t)} = [P_t; X_{\text{original}}]
$$
对于一组任务 $\{T_1, T_2, ..., T_k\}$，不是每个任务都插入全量 LoRA，而是仅选择部分任务共享相同的 LoRA 模块。每个 LoRA 插入位置通过一个 task-aware gate 来控制：
$$
\Delta W = s_t \cdot A B
$$
其中，$s_t \in [0,1]$：是一个与任务相关的权重，用来控制这层 LoRA 是否参与；A/B 是共享的低秩矩阵。

**总结**：AdaPtLoRA 结合了 Prompt Tuning 与 LoRA 的优势，允许模型为每个任务引入一个轻量 prompt，同时自适应地选择哪些 Transformer 层插入 LoRA，并支持多任务共享。它显著降低了训练参数量，同时增强了任务适应性和泛化能力，是当前多任务 PEFT 的一个代表方法。

- **QLoRA**

QLoRA = Quantized + LoRA，把原模型做 4bit 权重量化（不训练），在其 Frozen 权重上插入可训练的 LoRA 模块，只训练极小的新增参数。4-bit 量化（NF4 格式）。





### 16. 讲一下 Prompt Tuning

提示微调（Prompt Tuning）是一种参数高效微调（PEFT）方法，它通过在输入中添加一小段可学习的提示向量（prompt embeddings），而不是微调整个预训练模型，从而实现任务适应。它只微调一小部分“提示向量”，冻结原始模型参数。

提示微调的核心是：在输入文本前添加一段可学习的、任务相关的“软提示（soft prompt）”，并仅训练这部分参数。例如：

```
Input:  [P1] [P2] [P3] ... [Pn] + "Translate English to French: Hello"
```

- `[P1]...[Pn]` 是可训练的嵌入向量（prompt embeddings）。
- 这些向量和普通词嵌入具有相同的维度（如 768 或 1024）。
- 训练时，仅更新这些提示向量，其它模型参数保持冻结。

 它与手工 Prompt（Hard Prompt）不同，手工 Prompt 靠人写的自然语言提示；而提示微调是一组向量，不对应具体单词，完全数据驱动、可学习。

提示微调训练流程如下：

1. **初始化 Prompt 向量**：随机初始化，或从已有词嵌入中初始化（如取“good”、“great”这样的向量）。
2. **构建模型输入**：将这些提示向量拼接在输入 token embeddings 前。
3. **训练**：冻结原模型，仅更新 prompt 向量。
4. **推理**：使用训练好的 prompt 向量，与实际任务文本拼接后喂入模型。

优势是参数量极低，适合多任务学习。





### 17. 讲一下 Prefix Tuning 和 P-Tuning

**Prefix Tuning** 在每一层 Transformer 的注意力模块前添加一段“可学习的前缀向量”；**P-Tuning** 在输入 embedding 层前插入一组“可训练的 prompt 嵌入”，甚至可引入深层结构。

- **Prefix Tuning**

在每层 Transformer 的 self-attention 模块中，在 key 和 value 向量前加入可学习的“前缀向量”，其形式如下：

```
Q, K, V ← [prefix_K; real_K], [prefix_V; real_V]
```

这样做不修改输入文本本身；不修改主干模型参数；通过“引导注意力”的方式控制模型生成/理解行为。更加适合decoder 模型进行生成类任务（如对话、摘要）。

- **P-Tuning**

提出连续提示（Continuous Prompts），并通过轻量级神经网络（如双向LSTM或MLP）生成提示的嵌入。这种参数化方式能捕捉提示token间的依赖关系，使提示更灵活且任务适配性更强。

具体步骤为：每个任务或类别先用一个 token 向量标识，将这些 token 输入网络，输出最终的 prompt embedding；将输出拼接到真实输入 embedding 前。参数量也很少，通常几万到几十万。





### 18. 讲一下 Adapter 微调（Adapter Tuning）

Adapter 微调指的是：在预训练模型（如 BERT、GPT）的每一层之间插入一个小的可训练模块（adapter），并在微调时只更新这些 adapter 模块，而保留原始模型参数不变。

每层 Transformer 原本结构是这样：

```
[Input] → Attention → FeedForward → [Output]
```

插入 Adapter 后变成：

```
[Input] → Attention → FeedForward → Adapter → [Output]
```

Adapter 是一个极其轻量的小网络，通常由两层全连接构成：

```
Adapter(x) = W_up (ReLU(W_down(x)))
```

其中：`W_down` 把维度从 `d_model` 降到 `bottleneck_dim`（比如 768 → 64）；`W_up` 把维度从 `bottleneck_dim` 再升回 `d_model`

训练参数量很少，不改变主干模型的参数，通常几十万到几百万。支持多任务切换（每个任务加载不同 adapter 即可）。







