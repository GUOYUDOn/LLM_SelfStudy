### 1. 什么是 FlashAttention?

FlashAttention 是一种针对 Transformer 模型中注意力机制（Attention）的高效计算优化方法。传统 attention 的问题主要有：显存占用高，显存需求与序列长度平方成正比；计算速度慢：频繁读写高带宽内存（HBM）导致延迟，尤其在 GPU 上效率低下。

核心技术：

- **分块计算（Tiling）**：将大的注意力矩阵分割为小块（Tiles），在 SRAM（共享内存）中逐块计算，避免一次性加载整个矩阵到 HBM。将输入序列 X 按序列长度分块，每块大小为 $N_{block}$。对于每个 $X_i$，分别计算 $Q_i$，$K_i$，$V_i$，通过计算得到最终结果：

$$
O_{i,j}^{(h)} = \text{softmax}(S_{i,j}^{(h)}) \cdot V_j^{(h)} \in \mathbb{R}^{B \times N_{block} \times d_k}
$$

​	并将结果逐步累加。计算某块的 softmax 时，需考虑之前块的最大值，进行重缩放（Rescaling），实现数值	稳定。

- **重计算（Recomputation）**：在前向传播中不保存中间矩阵（如 $QK^T$ 和 softmax 结果），而是在反向传播时动态重新计算这些中间结果。以时间换空间，减少显存占用。
- **融合操作（Kernel Fusion）**：将多个计算步骤（如矩阵乘法、softmax、掩码应用）合并为单个 CUDA 内核（Kernel），减少 HBM 访问次数。





### 2. 大模型是如何对 MHA 进行优化的？

- **多查询注意力（MQA）**：所有头共享同一组 $K$ 和 $V$ 矩阵，仅保留 $Q$ 的多头结构。将 $K/V$ 的参数量从 $h \times d_{model} \times d_k$ 降低至 $1 \times d_{model} \times d_k$。推理时 KV Cache 体积缩小，但表达能力受限，影响性能。

-  **分组查询注意力（GQA）**：在 MHA 和 MQA 之间寻求平衡，保留 MHA 的表达能力，降低 MQA 的性能损失，通过分组共享减少 $K/V$ 计算量。将 $H$ 个头分为 $G$ 组，每组共享 $ K/V$ 矩阵。对于组 $g$，每个头分别计算 $Q$，而所有头共享相同的 $K$ 和 $V$。对于属于组 $g$ 的注意力头 $h$，其注意力计算如下：
  $$
  \text{Attention}_h = \text{softmax} \left( \frac{Q_h K_g^\top}{\sqrt{d_k}} \right) V_g \quad (\text{其中 } h \in \text{组 } g)
  $$

​	$Q_h \in \mathbb{R}^{B \times L \times d_k}$，$\text{softmax}(Q_h K_g^\top / \sqrt{d_k}) \in \mathbb{R}^{B \times L \times L}$，乘以 $V_g \in \mathbb{R}^{B \times L \times d_v}$ 得到：
$$
\text{Attention}_h \in \mathbb{R}^{B \times L \times d_v}
$$
​	最终，将 $H$ 个这样的张量拼接，得到输出。

- **多头潜在注意力（Multi-head Latent Attention，MLA）**：对 attention 的 key 和 value 进行低秩联合压缩，从而在推理过程中减少 Key-Value (KV) 缓存的开销。





### 3. 详细讲解一下 MLA

MLA 的核心思想是对 attention 的 **key 和 value 进行低秩联合压缩**，从而在推理过程中减少 Key-Value (KV) 缓存的开销。下面分三个步骤进行讲解。

#### 1）对KV的处理

$h_t ∈ \mathbb{R}^d$ 表示第 t 个 token 在某一注意力层的输入，我们需要对其进行变化，转换成 $KV$ 并进行缓存处理。

- 首先对 $h_t$ 进行压缩（下投影），生成压缩后的潜变量 $c_t^{KV}$，投影的矩阵记作 $W^{DKV}$。
- 还原 $K$：使用上投影矩阵 $W^{UK}$作用于潜变量，得到 $k_t^C$，通过 reshape 变成多头的 $k_{t,i}^C$。
- 还原 $V$：使用上投影矩阵 $W^{UV}$ 作用于潜变量，得到 $v_t^C$ ，通过 reshape 变成多头的 $v_{t,i}^C$。
- 得到带有旋转位置编码的 $K$：对原始向量表示 $h_t$ 进行变换，得到 $k_t^R=\text{RoPE}(W^{KR}h_t)$。
- 合并 $K$：注意上面的 $k_t^R$ 是不分多头，也就是说是所有头共享的。因此，对于第$i$个头，$k_{t,i}=[k_{t,i}^C;k_t^R]$。

#### 2) 对Q的处理

为了减少训练过程中的激活内存开销，对 Query 同样也进行了低秩压缩。

- 首先对 $h_t$ 进行压缩（下投影），生成压缩后的潜变量 $c_t^{Q}$，投影的矩阵记作 $W^{DQ}$。
- 再对 $c_t^{Q}$ 进行上投影还原，$q_{t}^C=W^{UQ}c^Q_t$，通过 reshape 得到多头的 $q_{t,i}^C$。
- 生成带有旋转位置编码的 $Q$：对于 $Q$ 的潜变量进行处理，$q_t^R=\text{RoPE}(W^{QR}c_t^Q)$，结果同样要 reshape 到每个头上，记作 $q_{t,i}^R$。
- 合并 $Q$：对于第$i$个头，$q_{t,i}=[q_{t,i}^C;q_{t,i}^R]$。

#### 3）计算 Attention 并拼接

- 对于第$i$个头，当前$t$​的 Attention 输出为：
  $$
  o_{t,i} = \sum_{j=1}^{t} \text{Softmax}_j\left( \frac{q_{t,i}^\top k_{j,i}}{\sqrt{d_h + d_{h}^R}} \right) \cdot v_{j,i}^C
  $$
  其中 $d_h + d_{h}^R$ 表示的是 $q_{t,i}$ 和 $k_{j,i}$ 的维度。

- 最后将所有头拼接后做线性投影输出：
  $$
  u_t = W^O \cdot [o_{t,1}; o_{t,2}; ...; o_{t,n_h}] \in \mathbb{R}^d
  $$
  $W^O \in \mathbb{R}^{d \times (n_h \cdot d_h)}$.

#### 重点关注

需要关注的是，在进行 KV 缓存的时候，我们仅需要存储 KV 的潜变量形式 $c_t^{KV}$ 以及旋转后的 $k_t^R$ 即可。这将显著减少 KV 缓存，并几乎不影响性能。

对于旋转位置编码部分，$K$ 和 $Q$ 采用的方式截然相反。$K$ 对原始输入进行投影并旋转，不区分每个头的区别；而 $Q$ 则是对其潜变量形式进行旋转，并 reshape 到不同头上。

全流程中，对于 $QKV$ 的处理大体可分为**压缩**、**上投影**和**旋转**三个部分。算上最终的线性投影矩阵 $W^O$，每层需要训练的矩阵总数量为**8**个。





### 4. 讲一下 RLHF

RLHF：Reinforcement Learning from Human Feedback，**从人类反馈中进行强化学习**。其核心思想为：用人类偏好来训练模型，让它学会什么样的回答更好，然后通过强化学习最大化“被人类喜欢”的回答概率。

- **第一阶段：SFT**

基础模型是从大规模语料上预训练好的（self-supervised），然后用人工标注的 prompt-ideal response 数据对模型进行微调，让它产生“比较好的初始行为”。训练方式是标准的交叉熵损失。

- **第二阶段：奖励模型训练**

人类评审员对多个模型回答打分（比如两个回答中哪个更好），收集成对偏好数据：
$$
(A, B), \quad \text{标签：prefer A over B}
$$
用这些对比数据训练一个 Reward Model $R_\theta$，输入模型的回答，输出一个分数：$R_\theta(\text{response})$。

- **第三阶段：PPO**

使用强化学习算法，最常见是 Proximal Policy Optimization (PPO)。用前面训练好的奖励模型 $R_\theta$ 作为“奖励函数”，当前语言模型是一个策略模型 $\pi_\phi$，目标是最大化奖励：
$$
\max_\phi \mathbb{E}_{\text{response} \sim \pi_\phi} [R_\theta(\text{response})]
$$
同时需要限制模型的行为偏离 SFT 模型太远，避免发散（通过 KL 惩罚）。





### 5. 讲一下 Reward Model

Reward Model 是一个模型，用来预测某个回答的「人类偏好得分」，越高表示人类越可能喜欢这个回答。

- **数据来源**

（prompt, response_1, response_2, preference），其中`preference ∈ {0, 1}` 表示哪一个回答更好。可以采用人类直接打分，使用模型打分，从已有数据中构建等方式获取。

- **模型架构**

直接基于预训练语言模型，通常从 SFT 模型初始化，输入是：`prompt + response` 的拼接，输出不是预测下一个 token，而是一个标量得分 $r \in \mathbb{R}$，代表这段回答的好坏。具体来说，将输入序列拼接成`[BOS] prompt [SEP] response [EOS]`，模型输出最后一个 token 的 hidden state（或平均 pool），再接一个线性层，输出一个 scalar 值作为 reward 分数：
$$
r = \text{Linear}(h_{\text{last}})
$$

- **损失函数**

主要使用 pairwise preference loss（对比损失），也称 logistic loss。设置 $r_1 = R_\theta(\text{prompt}, \text{response}_1)$，$r_2 = R_\theta(\text{prompt}, \text{response}_2)$。对回答的分数进行二分类，如果 `preference=1`，损失函数为：
$$
\mathcal{L} = -\log \left( \frac{e^{r_1}}{e^{r_1} + e^{r_2}} \right)
$$
或写作：
$$
\mathcal{L} = \log \left( 1 + e^{-(r_1 - r_2)} \right)
$$

- **训练流程**

首先固定一个预训练模型，之后计算得分 $r_1$ 和 $r_2$，并使用损失函数更新 RM 的参数。在训练完成后，RM 就可以在 RL 阶段提供奖励信号。





### 6. 讲一下 PPO 中的 Value Model

Value Model 在 PPO 中的主要作用是估计某个状态 s 的值函数 V(s)，即从当前状态出发，在未来能获得的期望累计奖励。用于后续计算 Advantage，并作为策略更新的基准参考，帮助控制学习的方差。

- **模型架构**

Value Model 通常和策略模型共享 Transformer backbone，然后加一个 **value head**（线性层）输出**每个 token** 的 V 值。它的输入是一段完整的文本（Prompt + Response），对每个位置的 token 输出一个状态值 $V(s_t)$。

- **简单模型流程**

将 Prompt 与策略模型的输出拼接，作为 Value Model 的输入，生成一个逐 token 的 value 序列 $V_1,V_2,...$ ，将 RM 的 reward 广播到整个序列，并计算优势函数（Advantage）：$A_t = R_t - V(s_t) = R - V(s_t)$，利用这个优势去训练 PPO 的策略模型。

对于 Value Model 自身，我们使用 **MSE** 作为损失函数进行训练：
$$
\mathcal{L}_{\text{value}} = \mathbb{E}_t \left[ \left( V(s_t) - R \right)^2 \right]
$$
求优势的时候，可以使用 GAE 进行优化。



### 7.讲一下 PPO 的完整流程

PPO（Proximal Policy Optimization）近端策略优化，是一种稳定高效的强化学习方法，用于在 RLHF 中微调语言模型，使其生成更符合人类偏好的回答。PPO 能够在不偏离原模型太远的前提下，最大化奖励模型输出的评分（人类偏好）。概括一下可以分为 采样、反馈、学习三个部分。

![image-20250327234029089](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20250327234029089.png)

- **前期准备**

策略模型（Policy）：$\pi_\phi$ —— 当前我们要训练的语言模型

初始模型（Reference Policy）：$\pi_{\text{ref}}$ —— SFT 阶段冻结下来的旧模型（当作行为参考）

奖励模型：$R_\theta$ —— 评估生成文本好坏（来自上一步 RM 的结果）

价值模型（Value Model）—— 估计策略模型在某一步的期望回报，用来计算 Advantage

- **训练目标**

在强化学习框架下，我们希望优化语言模型的参数 $\phi$，使其在生成回应 $y \sim \pi_\phi(\cdot | x)$ 时能最大化奖励：
$$
\max_\phi \ \mathbb{E}_{x, y \sim \pi_\phi}[R_\theta(x, y)] \quad \text{s.t.} \quad D_{KL}(\pi_\phi \parallel \pi_{\text{ref}}) \text{ 不太大}
$$

- **1. 采样**

从数据集中采样一批 prompt，用当前策略模型 $\pi_\phi$ 生成 response（也叫 trajectory）：
$$
x = \text{prompt},\quad y \sim \pi_\phi(\cdot | x)
$$

- **2. 计算奖励**

考虑加入 KL penalty 的情况，计算该回答的奖励分数（Reward）。我们采用对数比来替代传统的 KL 散度。

如果当前 token $y_t$ 不是最后一个 token（即 $t < T$）：
$$
R_t = -\beta \cdot \left( \log \pi_\phi(y_t \mid x, y_{<t}) - \log \pi_{\text{ref}}(y_t \mid x, y_{<t}) \right)
$$
如果是最后一个 token（即 $t = T$），还要加上奖励模型的评分：
$$
R_T = R_\theta(x, y) - \beta \cdot \left( \log \pi_\phi(y_T \mid x, y_{<T}) - \log \pi_{\text{ref}}(y_T \mid x, y_{<T}) \right)
$$
这些 per-token reward 将用于训练 Value Model 和计算 Advantage，是 PPO 优化的目标信号。

- **3. 训练 Value Model（Critic）**

我们需要对 Value Model 进行训练。它是在 RL loop 中和 Policy 同步训练的。首先构建 Value Model 的输入。对每个生成的序列 $(x, y)$，我们会把它切成：

$$
x, y_{\leq t} \quad \text{for each } t = 1, ..., T
$$
因此输入是：prompt + 当前 prefix（前 $t$ 个 token）。对于每个时间步 $t$，输出一个 scalar：

$$
V_\psi(x, y_{1:t}) \approx \mathbb{E}[\text{未来总 reward}]
$$

最终利用最小二乘 loss（MSE），我们令所有时间步都拟合最终 reward $r$：
$$
\mathcal{L}_{\text{value}} = \frac{1}{T} \sum_{t=1}^T \left(V_\psi(x, y_{1:t}) - R_t \right)^2
$$

- **4. 计算 Advantage **

用 reward 减去 baseline（来自 Value Model）：
$$
A_t = R_t - V_\psi(x, y_{1:t})
$$
可以使用 **GAE**（Generalized Advantage Estimation）进一步平滑。

- **5. 更新策略模型**

对于生成的每个 token，计算当前策略相对旧策略的概率比（有时候用 $\pi_{old}$ 代替 $\pi_{ref}$，迭代更稳定)：
$$
r_t = \frac{\pi_\phi(y_t | x, y_{<t})}{\pi_{\text{ref}}(y_t | x, y_{<t})}
$$
PPO 的核心 loss（需要最小化）：
$$
\mathcal{L}_{\text{PPO}} = \mathbb{E}_t \left[
\max\left(
- r_t A_t,\;
- \text{clip}(r_t,\ 1 - \epsilon,\ 1 + \epsilon)\, A_t
\right)
\right]
= \frac{1}{T} \sum_{t=1}^{T} \max\left(
- r_t A_t,\;
- \text{clip}(r_t,\ 1 - \epsilon,\ 1 + \epsilon)\, A_t
\right)
$$
如果策略更新太猛（$r_t$ 偏离 1 太多），clip 会抑制它，这样避免训练不稳定、模型发散。

- **6.  同步训练 Policy + Value Model**

每一轮迭代中，同时优化两个模型：1）用上面的 PPO loss 优化策略模型参数 $\phi$；2）用 MSE loss 优化 value model 参数 $\psi$。

PPO 中要求 Critic 能及时估计当前 policy 下的价值，所以它和 policy 改动步调不能差太多，否则 Advantage 计算会失真。有时会对 value loss 的更新也加上一个 clip，或者限制更新幅度；另一些实现会设计额外技巧，防止价值函数过拟合导致 Advantage 不准确。

- **注意事项**

1. 在一些研究中，会区分 **旧策略** 和 **参考模型**。对于 KL 散度的计算，会使用参考模型，不随时间改变；但在clip ratio中，有的地方使用的是上一次迭代得到的策略副本。

2. **KL Penalty** 的实现方式在实际中可能会采用 **对数比** 作为替代： ：
   $$
   \beta \sum_{t=1}^{T} \bigl(\log \pi_{\phi}(y_t \mid x,y_{<t}) - \log \pi_{\mathrm{ref}}(y_t \mid x,y_{<t})\bigr).
   $$
   这个与 KL 约等价，但实现起来更加方便。

3. 上面提到的对数比中，$y_t$ 是策略模型生成的值，并不一定是 softmax 后概率最高的词。对于第 $t$ 个 token，我们直接计算它最终的 log-softmax 向量，得到的应该是与词汇表大小一致的向量，然后选择对应生成的实际 token 的值，获取他的 id。对旧策略模型做同样 log-softmax 处理后，直接取对应 id 的值即可。

   给更新策略模型用的 $r_t$ 也是通过类似的方法得到，不过用的是 softmax 而不是 log-softmax。

4. PPO loss 中，$r_t A_t$ 是正常的 **policy gradient**。如果 $r_t > 1$，表示策略变得更偏爱这个动作（概率变大了）；如果 $A_t > 0$，这个动作是好的，就应该奖励它（增大概率）。所以 $r_t A_t$ 是想要鼓励/惩罚动作的原始梯度方向。$\text{clip}(r_t, 1-\epsilon, 1+\epsilon) \cdot A_t$ 是保险措施，如果 $r_t$ 变动太大（偏离旧策略太远），就将它限制，即使 Advantage 很大，也不能让策略更新太剧烈。$\epsilon$ 通常取 0.2。

5. **GAE**：对时序差分估计（TD）进行改进，平衡 bias 和 variance，构造平稳且可学习的 Advantage。核心公式：
   $$
   A_t^{\text{GAE}} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
   \quad \text{其中} \quad
   \delta_t = R_t + \gamma V(s_{t+1}) - V(s_t)
   $$
   超参数：$\gamma, \lambda$，通常取 0.99, 0.95。$t=0,1,...,T−1$，实际中要倒着计算，只对 response 的有限 token 计算。

6. 在实际中有时候会写：
   $$
   \mathcal{L} = \mathcal{L}_{\text{PPO}} + \alpha \cdot \mathcal{L}_{\text{value}}
   $$
   然后进行统一的梯度回传。这个要求 Actor 和 Critic 共享同一个主干，在主干网络之后，策略模型连接用于输出 token logits 的线性层，价值模型连接用于输出 value 即 $V(s_t)$ 的层，这样就可以一起进行训练。





### 8. 介绍一下 DPO

**DPO**（Direct Preference Optimization）是一种用于对齐语言模型的训练方法，目标是在不依赖强化学习的前提下，直接优化模型对人类偏好的拟合能力。它通过一种巧妙的设计思路将强化学习的最大化奖励和转化为损失函数，使得可以直接基于监督学习进行训练。

DPO 的核心在于一个**对比损失函数**，本质上是最小化以下目标：
$$
\mathcal{L}(\theta) = -\log \frac{\exp(\beta \cdot \text{log } \pi_\theta(y^+|x))}{\exp(\beta \cdot \text{log } \pi_\theta(y^+|x)) + \exp(\beta \cdot \text{log } \pi_\theta(y^-|x))}
$$
其中：$\pi_\theta(y|x)$ 是当前模型对输出 y 的条件概率；$\beta$ 是一个温度超参数，用于控制对比强度。

需要注意的是，对于一个给定的 prompt $x$，和一个回复 $y = (y_1, y_2, \dots, y_T)$，语言模型定义的条件概率是：
$$
\pi_\theta(y \mid x) = \prod_{t=1}^{T} \pi_\theta(y_t \mid x, y_{<t})
$$
对这个概率取对数，就得到：
$$
\log \pi_\theta(y \mid x) = \sum_{t=1}^{T} \log \pi_\theta(y_t \mid x, y_{<t})
$$
这就是我们用于 DPO 的 reward：一个完整回复的 log-likelihood 总和。对于大量样本进行 DPO 时，我们有：
$$
\mathcal{L}_\text{DPO}(\theta) = \frac{1}{N} \sum_{i=1}^N \mathcal{L}_i(\theta)
= \frac{1}{N} \sum_{i=1}^N -\log \frac{\exp(\beta \log \pi_\theta(y_i^+ \mid x_i))}{\exp(\beta \log \pi_\theta(y_i^+ \mid x_i)) + \exp(\beta \log \pi_\theta(y_i^- \mid x_i))}
$$
在实际中，$\beta$ 通常取 0.1。训练过程中，除了损失之外还有：接受/拒绝的 Reward，accuracies（有多少比例的$r(x, y^+) > r(x, y^-)$），平均的 reward 差值，句子级 log-prob（包括正负样本）以及 logit 值。





### 9. DPO 的数学推导

对于偏好数据集，我们有 $\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N$，为了不对奖励模型进行训练，我们希望参数化奖励模型 $r_\phi(x, y)$，并通过最大似然估计参数值。我们假设 winner $y_1$ 比 loser $y_2$ 更被偏好，那么我们希望：
$$
p(y_1 > y_2 \mid x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))}
$$
其中 $r(x,y_i)$ 是分数。我们对所有训练样本都最大化 $p(y_w > y_l \mid x)$，即最小化：
$$
\mathcal{L}_R(r_\phi, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\left( r_\phi(x, y_w) - r_\phi(x, y_l) \right) \right]
$$
其中：

- $\sigma(\cdot)$ 是 sigmoid 函数；
- $r_\phi(x, y_w) - r_\phi(x, y_l)$ 是两个输出的 reward 差；
- log-sigmoid 的形式就是二分类交叉熵损失的对数形式（类似 Logistic Regression）：

$$
\log \sigma(a) = -\log(1 + e^{-a})
$$

从 PPO 的算法可知，我们想要在不偏离 SFT 过多的情况下最大化生成文本获得的奖励和，即：
$$
\max_\pi \, \mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi} \left[ r(x, y) \right] - \beta D_{\text{KL}} \left[ \pi(y|x) \| \pi_{\text{ref}}(y|x) \right]
$$
其中，$r(x, y)$ 为奖励函数，$\pi_{\text{ref}}(y|x)$ 为参考模型（一般为 SFT 后的模型），$\pi(y|x)$ 为我们要优化最终得到的生成模型。对公式进行化简：
$$
\begin{aligned}
\max_\pi \, &\mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi} \left[ r(x, y) \right] - \beta D_{\text{KL}} \left[ \pi(y|x) \| \pi_{\text{ref}}(y|x) \right] \\
= \, &\max_\pi \, \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y|x)} \left[ r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} \right] \\
= \, &\min_\pi \, \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y|x)} \left[ \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta} r(x, y) \right] \\
= \, &\min_\pi \, \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathbb{E}_{y \sim \pi(y|x)} \left[ \log \frac{\pi(y|x)}{\frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r(x, y)\right)} \right] - \log Z(x) \right]
\end{aligned}
$$
下面我们可以定义函数 $\pi^*(y \mid x)$ 为一个有效的概率分布：
$$
\pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)
$$
其中，$\pi^*(y \mid x)$ 是一个有效的概率分布，原因解释如下：

- $\pi^*(y \mid x) \geq 0$，对所有 $y$ 成立
- $\sum_y \pi^*(y \mid x) = 1$

不难发现，我们可以将 $Z(x)$ 看作一个归一化常数（虽然它与 $x$ 有关），它可以确保 $\pi^*(y \mid x)$ 是一个概率分布。因此，我们可以继续化简：
$$
\begin{aligned}
\min_\pi \, \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathbb{E}_{y \sim \pi(y|x)} \left[ \log \frac{\pi(y \mid x)}{\frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)} - \right] \log Z(x) \right] \\
= \min_\pi \, \mathbb{E}_{x \sim \mathcal{D}} \left[ D_{\text{KL}}(\pi(y \mid x) \| \pi^*(y \mid x)) - \log Z(x) \right] \\
\Rightarrow \min_\pi \, \mathbb{E}_{x \sim \mathcal{D}} \left[ D_{\text{KL}}(\pi(y \mid x) \| \pi^*(y \mid x)) \right]
\end{aligned}
$$
由于 $Z(x)$ 不依赖于策略分布 $\pi(y \mid x)$，我们可以直接将其忽略。由 KL 散度的性质可知，**π(y∣x) 的最优策略即为：**
$$
\pi(y \mid x) = \pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)
$$
因此，不难得到奖励函数 $r(x,y)$ 的表达式：
$$
\begin{aligned}
\pi^*(y \mid x) &= \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left( \frac{1}{\beta} r(x, y) \right) \\
\Rightarrow \exp\left( \frac{1}{\beta} r(x, y) \right) &= \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} Z(x) \\
\Rightarrow r(x, y) &= \beta \log \left( \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} Z(x) \right) \\
\Rightarrow r(x, y) &= \beta \log \left( \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} \right) + \beta \log Z(x)
\end{aligned}
$$
对 $\pi^*$ 重新参数化，记作 $\pi_{\theta}$，将这个 $r_{\phi}(x,y)$ 带入到最上面的式子中（最小化损失）：
$$
\begin{aligned}
\mathcal{L}_R(r_\phi, \mathcal{D}) 
&= -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\left( r_\phi(x, y_w) - r_\phi(x, y_l) \right) \right] \\
&= -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} + \beta \log Z(x) - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} - \beta \log Z(x) \right) \right] \\
&= -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
\end{aligned}
$$
在实际计算中，还会进行简化：
$$
\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}
= \beta \left( \log \frac{\pi_\theta(y_w \mid x)}{\pi_\theta(y_l \mid x)} - \log \frac{\pi_{\text{ref}}(y_w \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right)
$$
在 DPO 中，我们忽略或固定 reference 模型，因此于是我们由：
$$
r_\phi(x, y) \propto \beta \cdot (\log \pi_\theta(y_w \mid x)-\log \pi_\theta(y_l \mid x))
$$
这等价于直接设置：
$$
r_{\phi}(x, y) := \beta \cdot (\log \pi_\theta(y_w \mid x)-\log \pi_\theta(y_l \mid x))
$$
对于 log-sigmoid，我们有下面公式：
$$
\text{log}\sigma(x)=-\log (1+ e^{-x})=\log(\frac{e^x}{1+e^x})
$$
带入公式，经过化简最终可得：
$$
\mathcal{L}_\text{DPO}(\theta) 
= -\log \left( \frac{\exp(\beta \log \pi_\theta(y_w \mid x))}{\exp(\beta \log \pi_\theta(y_w \mid x)) + \exp(\beta \log \pi_\theta(y_l \mid x))} \right)
$$



### 10. 介绍一下PPO和DPO，他俩有什么区别？DPO比PPO好在哪里？DPO有什么缺点？

- **PPO** 是一种强化学习算法，常用于人类反馈学习（RLHF）阶段。它通过训练一个 reward model（根据人类偏好得分），并用该 reward 对语言模型进行 policy gradient 优化。

  - 优化目标：
    $$
    \max_\pi \mathbb{E} [r(x, y)] - \beta \cdot \text{KL}(\pi \| \pi_{\text{ref}})
    $$

  - 核心是限制策略更新的“步长”（proximal），防止模型偏离太快。

- **DPO** 是一种**直接优化偏好数据（preference pairs）**的方法，完全不使用强化学习。DPO的训练目标是直接最大化偏好数据的似然，从而避免了PPO训练中复杂的奖励模型（Reward Model）估计和策略梯度计算过程，极大简化了人类偏好对齐的流程。构造了的对比损失函数为：
  $$
  \mathcal{L}_\text{DPO} = -\log \frac{\exp(\beta \log \pi(y^+|x))}{\exp(\beta \log \pi(y^+|x)) + \exp(\beta \log \pi(y^-|x))}
  $$
  其中 $y^+$ 是人类偏好的回答，$y^-$ 是被拒绝的回答。

- DPO 相比 PPO 的优势有：

  - **避免奖励模型误差**： DPO无需单独训练奖励模型，因此避免了因奖励模型的不准确性造成的误差和噪声问题，更直接、更准确地对齐人类偏好。

  - **训练更稳定、更简单**： DPO直接通过监督学习最大化偏好数据的概率，无需进行复杂的策略梯度估计，训练更加稳定、高效，且更易实现和调优。

  - **对数据利用效率高**： DPO充分利用已有的人类偏好数据，不需要大量的环境交互，显著节省了计算资源，且更适合数据受限场景。

- DPO 的缺点或局限：

  - 依赖优质的偏好对数据，如果偏好对数据质量差，DPO 无法学习出好的 reward 结构。

  - 无法显式建模 reward，而 PPO 可以 reuse reward model。
  - DPO 在优化过程中，有可能“相对地”提升了 chosen 相对 rejected 的概率差距，但同时两个的 absolute likelihood 都下降了，甚至 chosen 的 log-prob 比训练前还低。（可以增加正则项缓解）。
  - 泛化能力差，容易过拟合，难以应对多样性与探索性问题。





### 11. 什么是GRPO

GRPO（Group Relative Policy Optimization）群体相对策略优化，是一种强化学习的策略，是对 PPO 的改进。GRPO 直接使用多个采样输出的平均奖励作为Baseline，避免了传统PPO算法中对价值函数（value function）的依赖。在PPO中，需要训练一个价值函数来估计优势函数（advantage function），这增加了计算和内存负担。此外，在 LLMs 的上下文中，值函数在训练过程中被用作优势计算中的 Baseline，但通常只有最后一个 token 会被奖励模型赋予奖励分数，这可能使得值函数的训练变得复杂。而GRPO通过对同一问题生成多个输出，计算这些输出的平均奖励作为基线，从而简化了训练过程并减少了资源消耗。

![image-20250327222309394](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20250327222309394.png)

![image-20250327222329845](C:\Users\ROG\AppData\Roaming\Typora\typora-user-images\image-20250327222329845.png)

**训练流程**：

- **组内数据采样**

对每个输入 $x$，通过当前策略 $\pi_{\theta}$ 独立采样出 $n$ 个输出（即一个组）：
$$
y_i \sim \pi_{\theta}(y|x), \quad i=1,2,...,n
$$

- **计算奖励（Reward）**

利用奖励模型 $R$ 对每个生成的输出计算奖励：
$$
r_i = R(x, y_i), \quad i=1,2,...,n
$$

- **组内奖励标准化与优势计算**

计算组内奖励的均值 $\bar{r}$ 和标准差 $\sigma_r$：
$$
\bar{r} = \frac{1}{n}\sum_{i=1}^{n} r_i,\quad
\sigma_r = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(r_i-\bar{r})^2+\epsilon}
$$
其中 $\epsilon$ 是一个很小的常数，用于数值稳定性（如 $10^{-8}$）。

定义标准化后的奖励 $\hat{r}_i$：
$$
\hat{r}_i = \frac{r_i - \bar{r}}{\sigma_r}, \quad i=1,2,...,n
$$
此时优势函数（Advantage）$\hat{A}(x,y_i)$ 被定义为标准化后的奖励：
$$
\hat{A}(x,y_i) = \hat{r}_i
$$
这种优势估计方式不再需要单独的价值函数 $V(s)$。

- **计算目标函数**

GRPO的目标函数形式类似于PPO，不同之处在于直接在目标函数中加入KL散度惩罚项。GRPO目标函数定义为：
$$
L_{\text{GRPO}}(\theta) = \mathbb{E}_{x, y\sim\pi_{\theta_{\text{old}}}}\left[
\min\left(\rho(\theta)\hat{A}(x,y), \text{clip}\left(\rho(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}(x,y)\right)
- \beta \mathrm{KL}\left(\pi_{\text{ref}}(y|x)||\pi_{\theta}(y|x)\right)
\right]
$$
其中 $\rho(\theta)$ 为新旧策略概率的比值：
$$
\rho(\theta)=\frac{\pi_{\theta}(y|x)}{\pi_{\theta_{\text{old}}}(y|x)}
$$
$\epsilon$ 为PPO裁剪参数，通常设置如 $0.2$；$\beta$​ 为KL散度的惩罚系数，控制策略与参考模型的偏离程度。与传统PPO不同的是，GRPO使用了一种无偏的KL散度估计方法（证明简单）：
$$
\mathbb{D}_{\mathrm{KL}} \left[ \pi_{\theta} \| \pi_{\mathrm{ref}} \right] = \frac{\pi_{\mathrm{ref}}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - \log \frac{\pi_{\mathrm{ref}}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - 1,
$$
该值一定为正。

- **策略更新（Policy Update）**

与 PPO 不同，PPO 定义了损失函数，可以直接通过梯度回传更新参数；而在 GRPO 的目标函数中，优势函数本身取决于当前策略的输出，因此每个输出的优势值并非独立的，而是依赖于同一batch内其他样本的奖励值。这导致奖励到优势函数之间的关系不是逐样本独立的，因此需要显示的编写梯度计算过程：
$$
\theta \leftarrow \theta + \alpha\nabla_{\theta}L_{\text{GRPO}}(\theta)
$$
其中 $\alpha$ 是学习率。

- **过程监督**

结果监督仅在每个输出结束时提供奖励，这可能不足以有效监督复杂数学任务中的策略。过程监督可以在每个推理步骤结束时提供奖励。给定问题 q 和 G 个抽样输出 {o1, o2, ..., oG}，使用**过程奖励模型**（process reward model）对每个输出步骤进行评分，从而得到相应的奖励得到如下奖励集合：
$$
R = \left\{ \{r_{\text{index}(1)}^1, \dots, r_{\text{index}(K_1)}^1\}, \dots, \{r_{\text{index}(1)}^G, \dots, r_{\text{index}(K_G)}^G\} \right\}
$$
这里 $r_{\text{index}(j)}^i$ 表示第 $i$ 个输出中第 $j$ 步的奖励。$\text{index}(j)$ 表示第 $j$ 步的终结标记（step ending token）在整个序列中的索引。$K_i$ 是第 $i$ 个回答的总步骤数。

之后，同样进行标准化处理：
$$
\tilde{r}_{\text{index}(j)}^i = \frac{r_{\text{index}(j)}^i - \mathrm{mean}(R)}{\mathrm{std}(R)}
$$
接下来，过程监督计算每个标记的优势作为后续步骤的标准化奖励之和，构造优势函数：
$$
\hat{A}_{i,t} = \sum_{\text{index}(j) \geq t} \tilde{r}_{\text{index}(j)}^i
$$

- **迭代强化学习**

在强化学习的训练进程中，随着策略模型的不断进化，旧的奖励模型可能不足以有效地监督当前的策略模型。因此，引入带有 GRPO 的迭代强化学习方法。在每一轮迭代中，基于当前策略模型采样新数据，用新策略生成的样本训练奖励模型。为了避免奖励模型“忘掉”以前的知识，同时增强泛化能力，每次训练奖励模型时，保留 10% 的历史数据，与最新数据一起混合训练。





### 12. GRPO 相比 PPO 有什么改进？为什么 DPO 不够还要引入 GRPO？

与 PPO 比较：

- 不需要单独的价值函数估计优势函数，避免价值函数带来的误差。

- 组内标准化奖励稳定训练过程，训练效率和稳定性显著提升。

- 计算资源需求降低，特别适合大规模语言模型微调。

不足在于：强烈依赖奖励模型质量，奖励模型误差可能放大影响。

与 DPO 比较：

- GRPO通过强化学习范式，更适合长周期决策问题。
- GRPO允许更好的探索-利用平衡，而 DPO 更加侧重于利用。





